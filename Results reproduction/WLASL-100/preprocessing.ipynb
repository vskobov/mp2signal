{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproduction of the results\n",
    "______\n",
    "***WLASL Dataset!***\n",
    "\n",
    "##### Please follow the procedures described on WLASL official web page: https://dxli94.github.io/WLASL/ . We recommend getting the full dataset from authors directly rather than doing the video downloads yourself \n",
    "\n",
    "##### You have to put all WLASL video samples (21095 files) in WLASL2000 directory and the cooresponding WLASL_v0.3.json file in WLASL-100 directory\n",
    "______\n",
    "\n",
    "We used Python 3.10.11\n",
    "\n",
    "Please make sure to install the requirements before executing this notebook: \n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "The reproduction flow:\n",
    "1. Obtaine the full WLASL dataset and place videos and JSON file in the directories mentioned above\n",
    "2. Run this notebook and perform preprocessing and training and test dataset creation\n",
    "3. Run the wlasl_ml.ipynb to train the model and reproduce the results\n",
    "4. Results might slightly be different due to the initial model seed on your hardware\n",
    "\n",
    "##### Preprocessing and reproducing WLASL results will take a lot of time, we recommend doing it on a separate machene  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'..')\n",
    "import mp2signal.mp2s as mp2s\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "videos_path = 'WLASL2000/'\n",
    "save_path = 'data/'\n",
    "fjs = open('WLASL_v0.3.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = json.load(fjs)\n",
    "\n",
    "vid = 0\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "with mp_holistic.Holistic(\n",
    "    smooth_landmarks=True,\n",
    "    model_complexity=2,\n",
    "    min_detection_confidence=0.1,\n",
    "    refine_face_landmarks=True,\n",
    "    min_tracking_confidence=0.1) as holistic:\n",
    "    for i in js:\n",
    "        text = i['gloss']\n",
    "        for s in i['instances']:\n",
    "            try:\n",
    "                vid_path  = videos_path + s['video_id']+ '.mp4'\n",
    "                \n",
    "                sample_m = mp2s.Movement(vid_path,False,holistic)\n",
    "                \n",
    "                pg = sample_m.posegram()\n",
    "                np.save(save_path+'raw_grams/'+ s['video_id'],pg)\n",
    "\n",
    "                basic_coords = sample_m._basic_normalization_mov_data(to_uint8=False)\n",
    "                basic_coords = basic_coords.astype(np.float16)\n",
    "                np.save(save_path+'basic_norm_grams/'+ s['video_id'],basic_coords)\n",
    "\n",
    "                del(pg)\n",
    "                del(basic_coords)\n",
    "                gc.collect()\n",
    "            except:\n",
    "                base_m = mp2s.Movement()\n",
    "                base_m_data = base_m.movement_from_mediapipe(videos_path + '45669' + '.mp4',False,holistic)\n",
    "                vid_path  = videos_path + s['video_id']+ '.mp4'\n",
    "\n",
    "                sample_m = mp2s.Movement()\n",
    "                sample_m_d = sample_m.movement_from_mediapipe(vid_path,False,holistic)\n",
    "                print(len(sample_m_d['MP_Face']))\n",
    "                if len(sample_m_d['MP_Face'])>=len(base_m_data['MP_Face']):\n",
    "                    sample_m_d['MP_Face'][:len(base_m_data['MP_Face'])] = base_m_data['MP_Face']\n",
    "                else:\n",
    "                    sample_m_d['MP_Face'] = base_m_data['MP_Face'][:len(sample_m_d['MP_Face'])]\n",
    "                sample_m.process(sample_m_d)\n",
    "                \n",
    "                pg = sample_m.posegram()\n",
    "                np.save(save_path+'raw_grams/'+ s['video_id'],pg)\n",
    "\n",
    "                basic_coords = sample_m._basic_normalization_mov_data(to_uint8=False)\n",
    "                basic_coords = basic_coords.astype(np.float16)\n",
    "                np.save(save_path+'basic_norm_grams/'+ s['video_id'],basic_coords)\n",
    "\n",
    "                del(pg)\n",
    "                del(basic_coords)\n",
    "                print('Faled face: ',i['gloss'], s['video_id'])\n",
    "            vid += 1\n",
    "            print('Total Progress ',(int((vid/21083)*100)),\"%\", str(vid)+\"/\"+ str(21083), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = 0\n",
    "data_pd = []\n",
    "class_i = 0\n",
    "for i in js:\n",
    "    text = i['gloss']\n",
    "    for s in i['instances']:\n",
    "        pg_path  = save_path +'raw_grams/'+ s['video_id']+ '.npy'\n",
    "        basic_norm_path = save_path +'basic_norm_grams/'+ s['video_id']+ '.npy'\n",
    "        pg = np.load(pg_path)\n",
    "        bngr = np.load(basic_norm_path)\n",
    "        set_split = s['split']\n",
    "        part_id = s['signer_id']\n",
    "        class_id = class_i\n",
    "        var_id = s['variation_id']\n",
    "        d = (set_split, text, part_id, class_id, var_id, bngr, pg)\n",
    "        data_pd.append(d)\n",
    "        del(pg)\n",
    "        vid += 1\n",
    "        print('Total Progress ',(int((vid/21083)*100)),\"%\", str(vid)+\"/\"+ str(21083), end='\\r')\n",
    "    class_i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_pd,columns=[ 'Set', 'Text', 'Participant ID', 'Class ID', 'Variation ID','Basic Norm PG', 'Posegram'])\n",
    "df = df.astype({'Set': 'category',\n",
    "                'Text': 'category',\n",
    "                'Participant ID': 'category',\n",
    "                'Class ID': 'category',\n",
    "                'Variation ID': 'string'})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(df,save_path+'df_wlasl_grams.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Posegram'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Basic Norm PG'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cut_posegram(posegram,i = 0):\n",
    "    posegram_trim = mp2s.trim_posegram(posegram)\n",
    "    pg = (torch.tensor(posegram_trim).T.float()-128) / 128\n",
    "\n",
    "    hands_detect = torch.zeros((posegram.shape[1]))\n",
    "    for fr in range(posegram.shape[1]-1):\n",
    "        hands_detect[fr] = 1 - abs(F.cosine_similarity(F.normalize(pg[fr][6:24],p=2,dim=-1),F.normalize(pg[fr][29:47],p=2,dim=-1),dim=0))\n",
    "        \n",
    "    points = np.where(hands_detect>0)[0]\n",
    "    \n",
    "    if len(points)>2:\n",
    "        start_point,furthest_point  = points[0], points[-1]  \n",
    "        cut_posegram = np.transpose(posegram)[start_point:furthest_point]\n",
    "        return  cut_posegram\n",
    "    else:\n",
    "        print(i, hands_detect)\n",
    "        return np.transpose(posegram)\n",
    "\n",
    "def cut_bngp(bnpg,posegram,i=0):\n",
    "    \n",
    "    posegram_trim = mp2s.trim_posegram(posegram)\n",
    "    pg = (torch.tensor(posegram_trim).T.float()-128) / 128\n",
    "\n",
    "    hands_detect = torch.zeros((posegram.shape[1]))\n",
    "    for fr in range(posegram.shape[1]-1):\n",
    "        hands_detect[fr] = 1 - abs(F.cosine_similarity(F.normalize(pg[fr][6:24],p=2,dim=-1),F.normalize(pg[fr][29:47],p=2,dim=-1),dim=0))\n",
    "        \n",
    "    points = np.where(hands_detect>0)[0]\n",
    "    \n",
    "    if len(points)>2:\n",
    "        start_point,furthest_point  = points[0], points[-1]  \n",
    "        cut_bnpg = np.transpose(bnpg)[start_point:furthest_point]\n",
    "        return  cut_bnpg\n",
    "    else:\n",
    "        print(i, hands_detect)\n",
    "        return np.transpose(bnpg)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "idx =0\n",
    "posegram = df['Posegram'].iloc[idx]\n",
    "\n",
    "cp = cut_posegram(posegram)\n",
    "cbn = cut_bngp(df['Basic Norm PG'].iloc[idx],df['Posegram'].iloc[idx],idx)\n",
    "#plt.figure(figsize=(5,5),dpi=200)\n",
    "plt.imshow(posegram,cmap='brg')\n",
    "plt.show()\n",
    "plt.imshow(cp,cmap='brg')\n",
    "plt.show()\n",
    "plt.imshow(df['Basic Norm PG'].iloc[idx],cmap='brg')\n",
    "plt.show()\n",
    "plt.imshow(cbn,cmap='brg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PosegramC'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BNPGC'] = list(cut_bngp(df['Basic Norm PG'].iloc[idx],df['Posegram'].iloc[idx],idx) for idx in range(len(df['Posegram'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BNPGC'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_sample(gram, factor):\n",
    "    gram = np.transpose(gram)\n",
    "    new_shape = list(gram.shape)\n",
    "    new_shape[1] = round(new_shape[1]*factor)\n",
    "    res_gram = np.zeros(new_shape,dtype=gram.dtype)\n",
    "    for i in range(gram.shape[0]):\n",
    "        res_gram[i] = np.interp(np.arange(0, round(gram[i].shape[0]*factor)), np.arange(0, gram[i].shape[0])*factor, gram[i])\n",
    "\n",
    "    res_gram = np.transpose(res_gram)\n",
    "    return res_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PosegramCR'] = list(resize_sample(p,100/p.shape[0]) for p in df['PosegramC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BNPGCR'] = list(resize_sample(p,100/p.shape[0]) for p in df['BNPGC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PosegramCRT'] = list(np.transpose(mp2s.trim_posegram(np.transpose(df['PosegramCR'].iloc[p]))) for p in range(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BNPGCRT'] = list(np.transpose(mp2s.trim_posegram(np.transpose(df['BNPGCR'].iloc[p]))) for p in range(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PosegramCRT'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(x):\n",
    "    plt.figure(dpi=200)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x,cmap='brg')\n",
    "    plt.show()\n",
    "\n",
    "idx = 6\n",
    "\n",
    "plot(df['Basic Norm PG'][idx])\n",
    "plot(df['BNPGC'][idx].T)\n",
    "plot(df['BNPGCR'][idx].T)\n",
    "plot(df['BNPGCRT'][idx].T)\n",
    "\n",
    "plot(df['Posegram'][idx])\n",
    "plot(df['PosegramC'][idx].T)\n",
    "plot(df['PosegramCR'][idx].T)\n",
    "plot(df['PosegramCRT'][idx].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(df,save_path+'df_wlasl_grams_cut_res_trim.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import pandas as pd \n",
    "\n",
    "def train_test_val(df,set_size, column, data_path, file_prefix):\n",
    "    tr_df = pd.DataFrame()\n",
    "\n",
    "    train_smlps = df.loc[(df['Set']=='train')&(df['Class ID']<set_size)][[column,'Class ID']]\n",
    "\n",
    "    tr_df['X'] = list([torch.from_numpy(train_smlps.iloc[p][0]) for p in range(len(train_smlps))])\n",
    "    #tr_df['X455'] = list([torch.tensor(train_smlps.iloc[p][1]-128).float()/128 for p in range(len(train_smlps))])\n",
    "    tr_df['Y'] = list([torch.tensor([train_smlps.iloc[p][1]]) for p in range(len(train_smlps))])\n",
    "\n",
    "    #print(len(tr_df))\n",
    "    #print(tr_df.head())\n",
    "\n",
    "    tr_df.to_pickle(data_path+file_prefix+'train'+str(set_size)+'.pkl')\n",
    "\n",
    "    ts_df = pd.DataFrame()\n",
    "\n",
    "    test_smlps = df.loc[(df['Set']=='test')&(df['Class ID']<set_size)][[column,'Class ID']]\n",
    "\n",
    "    ts_df['X'] = list([torch.from_numpy(test_smlps.iloc[p][0]) for p in range(len(test_smlps))])\n",
    "    #ts_df['X455'] = list([torch.tensor(test_smlps.iloc[p][1]-128).float()/128 for p in range(len(test_smlps))])\n",
    "    ts_df['Y'] = list([torch.tensor([test_smlps.iloc[p][1]]) for p in range(len(test_smlps))])\n",
    "\n",
    "    #print(len(ts_df))\n",
    "    #print(ts_df.head())\n",
    "\n",
    "    ts_df.to_pickle(data_path+file_prefix+'test'+str(set_size)+'.pkl')\n",
    "\n",
    "\n",
    "    val_df = pd.DataFrame()\n",
    "\n",
    "    val_smlps = df.loc[(df['Set']=='val')&(df['Class ID']<set_size)][[column,'Class ID']]\n",
    "\n",
    "    val_df['X'] = list([torch.from_numpy(val_smlps.iloc[p][0]) for p in range(len(val_smlps))])\n",
    "    #val_df['X455'] = list([torch.tensor(val_smlps.iloc[p][1]-128).float()/128 for p in range(len(val_smlps))])\n",
    "    val_df['Y'] = list([torch.tensor([val_smlps.iloc[p][1]]) for p in range(len(val_smlps))])\n",
    "\n",
    "    #print(len(val_df))\n",
    "    #print(val_df.head())\n",
    "\n",
    "    val_df.to_pickle(data_path+file_prefix+'val'+str(set_size)+'.pkl')\n",
    "\n",
    "    print('Done TRAIN TEXT VAL',set_size, column, data_path, file_prefix,len(tr_df),len(ts_df),len(val_df) )\n",
    "\n",
    "data_path = 'D:/WLASL/data/'\n",
    "\n",
    "#load from memory if needed\n",
    "#df = joblib.load(data_path+'df_wlasl_grams_cut_res_trim.joblib')\n",
    "\n",
    "df = df.astype({'Class ID': 'int'})\n",
    "\n",
    "train_test_val(df,100,'PosegramCRT',save_path,'')\n",
    "\n",
    "train_test_val(df,100,'BNPGCRT',save_path,'bnpg_')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
